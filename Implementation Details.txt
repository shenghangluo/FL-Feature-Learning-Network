FL NLC:

Tensorflow Implementation
1. The code is mainly implemented in Tensorflow. GRU and LSTM cells are implemented using Tensorflow default function: tf.contrib.rnn.GRUCell and tf.contrib.rnn.LSTMCell. Bi-directional RNN is implemented using Tensorflow tf.compat.v1.nn.bidirectional_dynamic_rnn. The dense layers are using Tensorflow tf.keras.layers.Dense.
2. The nonlinear activation function used in the hidden layer of the FNN is leaky-relu with alpha = 0.5. A dropout layer with a rate 0.5 is applied before the output layer.

Training
1. For each received symbol, we append 125 succeeding and preceding symbols together with the symbol of interest. Each symbol is consist of both real part and imaginary part for both polarization. Therefore, the training input feature has a shape of (batch_size, 251, 4)
2. Then we take the middle 25 time-steps of the output from the bi-directional RNN, and reshape it into a shape (batch_size, 25*hidden_unit), here the best hidden_unit we found is 25 for our simulation.
3. The learning rate used is 0.0001 and batch size is 512. The loss function is MSE

Testing
1. We take a testing data sequence of length 2^16 and append 100 transient symbols at the very beginning and end, which gives us a testing input feature shape of (2^16+200)
2. We firstly input this long testing data sequence into the trained network, and extract the output from the bi-directional GRU
3. Then we, instead of reshaping the output in Tensorflow, reshape the output and generate learned features for testing data in MATLAB, since the batch_size changes to 1 in testing mode, which made some troubles when we tried to do this in Tensorflow.
4. After it, we put the reshaped output back in Tensorflow to process it by the trained FNN, and gather the testing results.

Pruning
1. We firstly train the network to a converging point and then apply the pruning. 
2. We manually set a pruning schedule with a few pruning steps. At each pruning step, we set a pruning threshold and finetuning epoches. The finetuning epoches is calculated by (np.ceil(np.ceil(2.0 ** (-np.arange(pruning_iterations, 0, -1)) * total_iterations))), so that as the pruning going on, we can have more epoches to let the network finetune.
3. We take all the trainable weights in bi-directional GRU and apply masks to each trainable weights according to the corresponding threshold. The absolute value of the weight will be compared and the values that are lower than the threshold will be masked and corresponding gradient also will be masked.
4. By slowly increasing the pruning threshold we can finally prune 56% weights in bi-directional GRU

Quantization
1. It is implemented in MATLAB.
2. After we obtained the pruned network, we extract the learned features from bi-GRU for the whole testing data sequence. 
3. We also extract the trained weights and bias from the FNN network so that we can independently arrive the same results using MATLAB.
4. Then we apply k-means algorithm to the trained weight of the first layer in FNN since it takes the most complexity in FNN.
5. Given the first weight can be decomposed into two weights, we use MATLAB default function kmeans to cluster each decomposed weights.
6. Finally, we can achieve a quantization level at 75, given that the size of each weight originally is 625, for trained FNN.




FNN PB-NLC:

Pytorch Implementation
1. FNN PB-NLC is implemented according to the Nature paper entitled with "Field and lab experimental demonstration of nonlinear impairment compensation using neural networks"
2. The code is mainly implemented in Pytorch. FNN is implemented using Pytorch default function: nn.Linear. 
3. The nonlinear activation function used in the hidden layer is leaky-relu with alpha = 0.5 and a dropout layer with a rate 0.2 before the output layer.

Training
1. The learning rate used is 0.001 and batch size is 100. The loss function is MSE


Pruning
1. Similar as the pruning in FL NLC, but FNN is implemented in Pytorch instead of Tensorflow.
2. We extract the weight in the first hidden layer since it cost most. Then we apply a mask to the weight according to the threshold 




FNN-AM PB-NLC:

Pytorch Implementation
1. Similar as FNN PB-NLC, FNN-AM PB-NLC is also implemented in Pytorch
2. The code is similar to FNN PB-NLC, but we add an additional FNN to process the ICIXPM and SPM triplets.  

Training
1. The training is the same as FNN PB-NLC

Pruning
1. Similar as the pruning in FNN-AM PB-NLC, but FNN-AM PB-NLC takes another weight in the first layer of the second FNN into account. 
2. We extract the weights in the first hidden layer of the two FNN. Then we apply a mask to each weight according to the threshold 
